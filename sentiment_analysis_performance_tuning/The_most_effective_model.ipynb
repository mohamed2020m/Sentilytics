{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f91977-f3e3-4128-8954-9c533bb658b4",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3.75em;color:purple; font-style:bold\"><br>\n",
    "Introduction\n",
    "\n",
    "\n",
    "\n",
    "1. <strong>Objective</strong>: The goal of this analysis was to determine the most effective model and feature extraction technique for sentiment analysis.\n",
    "\n",
    "2. <strong>Feature Extraction Evaluation</strong>: We evaluated different feature extraction methods, including:\n",
    "   - <strong>Hashing TF-IDF + 1-Gram</strong>\n",
    "   - <strong>CountVectorizer TF-IDF + 1-Gram</strong>\n",
    "   - More complex features such as <strong>1-2-3-Grams</strong> and <strong>ChiSqSelector</strong>.\n",
    "\n",
    "3. <strong>Model Comparison</strong>: We compared the performance of the following models:\n",
    "   - <strong>Logistic Regression</strong>\n",
    "   - <strong>Naive Bayes</strong>\n",
    "   - <strong>SVM</strong>\n",
    "   - The comparison focused on precision across each scenario.\n",
    "\n",
    "4. <strong>MLflow Integration</strong>: We used <strong>MLflow</strong> to track our machine learning models, enabling us to:\n",
    "   - Record experiment metrics\n",
    "   - Identify the best combination of feature extraction method and model\n",
    "   - Optimize predictive accuracy through detailed experiment tacking.\n",
    "\n",
    "</p><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2445f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef81321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, NumericType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, StopWordsRemover, VectorAssembler\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import udf, col, count, when, isnull, countDistinct, length\n",
    "\n",
    "# Bibliothèques générales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# NLTK pour le traitement du texte\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# MLFlow\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83284138-2e5e-4abc-9b69-0ec045518780",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:1234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f951927",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Variables de contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc744576",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder\\\n",
    "            .master(\"local[*]\")\\\n",
    "            .appName(\"Sentiment_Analysis\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bb769-674c-45b4-a87d-650ebb76e6c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"./data.csv\"  # Path dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd30357",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Chargement du dataset et séparation train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a397a2-4535-49d8-81f7-3ffbd13210ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data schema\n",
    "schema = StructType([StructField(\"Sentence\", StringType(), True),\n",
    "    StructField(\"Sentiment\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535635a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark1.read.csv(path,\n",
    "                     inferSchema=True, # Spark uses the defined schema\n",
    "                     header=True,\n",
    "                     schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2591004-a204-4815-892c-8652ac3cd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les valeurs manquantes par colonne\n",
    "missing_values = df.select([count(when(col(c).isNull(), 1)).alias(c) for c in df.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67b509-1439-49c8-8034-97196073d326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7c06a-a83b-4ef0-9302-e504baf70b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la nouvelle colonne 'target'\n",
    "df = df.withColumn(\n",
    "    \"target\",\n",
    "    F.when(df[\"Sentiment\"] == \"neutral\", 0)\n",
    "     .when(df[\"Sentiment\"] == \"negative\", -1)\n",
    "     .when(df[\"Sentiment\"] == \"positive\", 1)\n",
    "     .otherwise(None)  # Gérer les autres valeurs (qui seront ignorées dans le filtrage)\n",
    ")\n",
    "# Création de la nouvelle colonne 'target'\n",
    "# Filtrer le DataFrame pour ne garder que les valeurs \"positive\", \"negative\" et \"neutral\"\n",
    "df = df.filter(df[\"Sentiment\"].isin(\"positive\", \"negative\", \"neutral\"))\n",
    "\n",
    "# Afficher les valeurs distinctes\n",
    "df.select(\"Sentiment\", \"target\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2abb16-31a6-4f53-a8f2-7efdb2566ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le Lemmatizer et le Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STOPWORDS.update(['rt', 'mkr', 'didn', 'bc', 'n', 'm', 'im', 'll', 'y', 've', \n",
    "                  'u', 'ur', 'don', 'p', 't', 's', 'aren', 'kp', 'o', 'kat', \n",
    "                  'de', 're', 'amp', 'will'])\n",
    "\n",
    "# Fonction de nettoyage du texte\n",
    "def clean_text(df, field):\n",
    "    # Remplacement des éléments à l'aide de F.regexp_replace\n",
    "    df = df.withColumn(field, F.regexp_replace(df[field], r\"http\\S+\", \" \"))  # Remplacer les URLs\n",
    "    df = df.withColumn(field, F.regexp_replace(df[field], r\"http\", \" \"))\n",
    "    df = df.withColumn(field, F.regexp_replace(df[field], r\"@\", \"at\"))\n",
    "    df = df.withColumn(field, F.regexp_replace(df[field], r\"#\\S+\", \" \"))  # Remplacer les hashtags\n",
    "    df = df.withColumn(field, F.regexp_replace(df[field], r\"[^A-Za-z(),!?@\\'\\\"_\\n]\", \" \"))  # Suppression des caractères spéciaux\n",
    "    df = df.withColumn(field, F.lower(df[field]))  # Convertir en minuscules\n",
    "    df = df.filter(col(\"Sentiment\").isin([\"negative\", \"positive\", \"neutral\"]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad5648-7d00-44ef-b7c8-c421c3733961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = clean_text(df, 'Sentence') \n",
    "df_pandas = df_cleaned.select('Sentiment').toPandas()\n",
    "\n",
    "# Tracer un countplot\n",
    "sns.countplot(x='Sentiment', data=df_pandas, palette='coolwarm')\n",
    "plt.title('Répartition des Sentiments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc805783-a334-43d2-9e41-49674791a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne avec la longueur des phrases\n",
    "df_cleaned = df_cleaned.withColumn('Sentence_Length', length(col('Sentence')))\n",
    "\n",
    "# Calculer la longueur moyenne des phrases par sentiment\n",
    "sentiment_length = df_cleaned.groupBy('Sentiment').agg({'Sentence_Length': 'avg'})\n",
    "\n",
    "# Renommer la colonne pour qu'elle soit plus facile à utiliser\n",
    "sentiment_length = sentiment_length.withColumnRenamed('avg(Sentence_Length)', 'Avg_Sentence_Length')\n",
    "\n",
    "# Convertir en Pandas pour l'utiliser avec Seaborn\n",
    "sentiment_length_pd = sentiment_length.toPandas()\n",
    "\n",
    "# Créer un boxplot pour la longueur des phrases par sentiment\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(x='Sentiment', y='Avg_Sentence_Length', data=sentiment_length_pd)\n",
    "plt.title('Longueur des phrases par sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Longueur moyenne de la phrase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d3145-3806-4aec-a2ff-deab1cce1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des sentences\n",
    "sentences = df.select('Sentence').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Générer le nuage de mots\n",
    "text = ' '.join(sentences)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f466154-cff2-44b5-90f8-c16ecf33645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les données pour chaque sentiment\n",
    "positive_sentences = df_cleaned.filter(df_cleaned['Sentiment'] == 'positive').select('Sentence').rdd.flatMap(lambda x: x).collect()\n",
    "negative_sentences = df_cleaned.filter(df_cleaned['Sentiment'] == 'negative').select('Sentence').rdd.flatMap(lambda x: x).collect()\n",
    "neutral_sentences = df_cleaned.filter(df_cleaned['Sentiment'] == 'neutral').select('Sentence').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Créer des word clouds pour chaque sentiment\n",
    "positive_wordcloud = WordCloud().generate(' '.join(positive_sentences))\n",
    "negative_wordcloud = WordCloud().generate(' '.join(negative_sentences))\n",
    "neutral_wordcloud = WordCloud().generate(' '.join(neutral_sentences))\n",
    "\n",
    "# Afficher les word clouds\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "plt.title('Positive Sentiment')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "plt.title('Negative Sentiment')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(neutral_wordcloud, interpolation='bilinear')\n",
    "plt.title('Neutral Sentiment')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b9ea3-6e6b-4f0b-a106-dbd40f3b4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en Pandas DataFrame\n",
    "df_pandas = df_cleaned.select('Sentiment', 'Sentence_length').toPandas()\n",
    "\n",
    "# Tracer un boxplot\n",
    "sns.boxplot(x='Sentiment', y='Sentence_length', data=df_pandas)\n",
    "plt.title('Longueur des Phrases par Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f666cb-34a3-4a81-918a-532e721958a7",
   "metadata": {},
   "source": [
    "<h1>Dataset statistics</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91266c9b-aff5-477b-b4a2-8fe268bb9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de variables (colonnes)\n",
    "num_variables = len(df.columns)\n",
    "\n",
    "# Nombre d'observations (lignes)\n",
    "num_observations = df.count()\n",
    "\n",
    "# Nombre de cellules manquantes\n",
    "missing_cells = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).collect()\n",
    "missing_cells_total = sum([row[c] for row in missing_cells for c in df.columns])\n",
    "\n",
    "# Pourcentage de cellules manquantes\n",
    "missing_cells_percentage = (missing_cells_total / (num_observations * num_variables)) * 100\n",
    "\n",
    "# Nombre de lignes dupliquées\n",
    "duplicate_rows = df.count() - df.distinct().count()\n",
    "\n",
    "# Pourcentage de lignes dupliquées\n",
    "duplicate_rows_percentage = (duplicate_rows / num_observations) * 100\n",
    "\n",
    "# Taille totale en mémoire du DataFrame\n",
    "total_size_memory = df.rdd.map(lambda row: sum([len(str(cell)) for cell in row])).sum()\n",
    "\n",
    "# Taille moyenne d'un enregistrement\n",
    "average_record_size = total_size_memory / num_observations\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"Number of variables: {num_variables}\")\n",
    "print(f\"Number of observations: {num_observations}\")\n",
    "print(f\"Missing cells: {missing_cells_total}\")\n",
    "print(f\"Missing cells (%): {missing_cells_percentage:.1f}%\")\n",
    "print(f\"Duplicate rows: {duplicate_rows}\")\n",
    "print(f\"Duplicate rows (%): {duplicate_rows_percentage:.1f}%\")\n",
    "print(f\"Total size in memory: {total_size_memory / 1024:.1f} KiB\")\n",
    "print(f\"Average record size in memory: {average_record_size:.1f} B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1beb8-b4ff-435e-b782-848cca3bba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df.count() - df.dropDuplicates().count()  # Nombre de doublons\n",
    "duplicate_rows_percentage = (duplicate_rows / num_observations) * 100\n",
    "\n",
    "# Données pour le graphique\n",
    "duplicate_data = {'Category': ['Duplicate Rows', 'Duplicate Rows (%)'], 'Count': [duplicate_rows, duplicate_rows_percentage]}\n",
    "\n",
    "# Création du graphique\n",
    "sns.barplot(x='Category', y='Count', data=duplicate_data)\n",
    "plt.title('Doublons dans les Données')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45610fd-e287-40e5-8c3b-e4c736252492",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_memory = df.rdd.map(lambda x: len(str(x))).sum()  # Estimation de la taille totale en mémoire\n",
    "average_record_size = total_size_memory / num_observations\n",
    "\n",
    "# Données pour le graphique\n",
    "size_data = {'Category': ['Total Size in Memory (KiB)', 'Average Record Size (B)'], 'Size': [total_size_memory / 1024, average_record_size]}\n",
    "\n",
    "# Création du graphique\n",
    "sns.barplot(x='Category', y='Size', data=size_data)\n",
    "plt.title('Taille en Mémoire des Données')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6ecc8-2bd2-4841-badd-37133dbaca26",
   "metadata": {},
   "source": [
    "<h1>Type de la variable</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9174e5-ec64-4a16-92a7-608462897b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_types = df.dtypes\n",
    "\n",
    "# Compter le nombre de variables numériques (NumericType)\n",
    "num_numeric_variables = len([col for col, dtype in columns_types if isinstance(df.schema[col].dataType, NumericType)])\n",
    "\n",
    "# Compter le nombre de variables catégorielles (StringType)\n",
    "num_categorical_variables = len([col for col, dtype in columns_types if isinstance(df.schema[col].dataType, StringType)])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f\"Number of numeric variables: {num_numeric_variables}\")\n",
    "print(f\"Number of categorical variables: {num_categorical_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9c7fc-a00e-4d58-99b9-9d4097e3fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # Remove links\n",
    "    text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    # Convert HTML references\n",
    "    text = re.sub('&amp', 'and', text)\n",
    "    text = re.sub('&lt', '<', text)\n",
    "    text = re.sub('&gt', '>', text)\n",
    "\n",
    "    # Remplacer certaines contractions par leurs formes complètes\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "\n",
    "    # Remplacer les caractères non alphabétiques par un espace\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]+', '', text)  # Supprimer les caractères non-ASCII\n",
    "    \n",
    "    \n",
    "    # Remove new line characters\n",
    "    text = re.sub('[\\r\\n]+', ' ', text)\n",
    "    \n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Remove multiple space characters\n",
    "    text = re.sub('\\s+',' ', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f1eed-4a6c-4319-97c3-6a52cfd93d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_udf = udf(pre_process, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c93eb0-b048-41d5-b0bc-823be5bf08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Processed_Sentence\", pre_process_udf(df[\"Sentence\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e490c48-1040-4174-87a0-7ef56323eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Processed_Sentence\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c5dbc-a3c3-4c84-ae83-0375beae1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29311b0-8c06-4d77-98a4-d86159abfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, test_set) = df.randomSplit([0.80, 0.20], seed = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a480115-9af0-4302-a5e2-732368c2b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les tailles des ensembles\n",
    "print(f\"Ensemble d'entraînement: {train_set.count()} exemples\")\n",
    "print(f\"Ensemble de test: {test_set.count()} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd7ea5-aaf1-4cf4-b8e4-0f618c03326a",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be6ad42",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (paramètres par défaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commencez une session MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Configuration du modèle\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    hashtf = HashingTF(inputCol=\"words\", outputCol='tf')\n",
    "    idf = IDF(inputCol='tf', outputCol=\"features\")\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])\n",
    "\n",
    "    # Enregistrez les hyperparamètres du modèle dans MLflow\n",
    "    mlflow.log_param(\"tokenizer\", \"Tokenizer\")\n",
    "    mlflow.log_param(\"hashtf\", \"HashingTF\")\n",
    "    mlflow.log_param(\"idf\", \"IDF\")\n",
    "    mlflow.log_param(\"classifier\", \"LogisticRegression\")\n",
    "\n",
    "    # Temps d'entraînement\n",
    "    start_time = datetime.utcnow()\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "    training_time = datetime.utcnow() - start_time\n",
    "    print('Training time:', training_time)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Enregistrez les métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Enregistrez le modèle entraîné dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")\n",
    "\n",
    "    # Enregistrez également le temps d'entraînement\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time.total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c36e1-2565-440f-a247-cc505f8c5e61",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (paramètres customisés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commencez une session MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Configuration du modèle avec HashingTF - IDF (paramètres customisés)\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "    idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5)  # minDocFreq: remove sparse terms\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])\n",
    "\n",
    "    # Enregistrez les hyperparamètres du modèle dans MLflow\n",
    "    mlflow.log_param(\"tokenizer\", \"Tokenizer\")\n",
    "    mlflow.log_param(\"hashtf\", \"HashingTF with custom params (numFeatures=2^16, minDocFreq=5)\")\n",
    "    mlflow.log_param(\"idf\", \"IDF with minDocFreq=5\")\n",
    "    mlflow.log_param(\"classifier\", \"LogisticRegression\")\n",
    "\n",
    "    # Temps d'entraînement\n",
    "    start_time = datetime.utcnow()\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    pipelineFit1 = pipeline.fit(train_set)\n",
    "\n",
    "    training_time = datetime.utcnow() - start_time\n",
    "    print('Training time:', training_time)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit1.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Enregistrez les métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Enregistrez le modèle entraîné dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit1, \"model\")\n",
    "\n",
    "    # Enregistrez également le temps d'entraînement\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time.total_seconds())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afc262-59ff-446f-813c-4faa36cd8487",
   "metadata": {},
   "source": [
    "## CountVectorizer - IDF (paramètres customisés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1086c-3915-4ac0-9b17-3617e2cba1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commencez une session MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Configuration du modèle avec CountVectorizer - IDF (paramètres customisés)\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "    idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)  # minDocFreq: remove sparse terms\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "\n",
    "    # Enregistrez les hyperparamètres du modèle dans MLflow\n",
    "    mlflow.log_param(\"tokenizer\", \"Tokenizer\")\n",
    "    mlflow.log_param(\"cv\", \"CountVectorizer with custom params (vocabSize=2^16, minDocFreq=5)\")\n",
    "    mlflow.log_param(\"idf\", \"IDF with minDocFreq=5\")\n",
    "    mlflow.log_param(\"classifier\", \"LogisticRegression\")\n",
    "\n",
    "    # Temps d'entraînement\n",
    "    start_time = datetime.utcnow()\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    pipelineFit3 = pipeline.fit(train_set)\n",
    "\n",
    "    training_time = datetime.utcnow() - start_time\n",
    "    print('Training time:', training_time)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit3.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Enregistrez les métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Enregistrez le modèle entraîné dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit3, \"model\")\n",
    "\n",
    "    # Enregistrez également le temps d'entraînement\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time.total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1f41f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## CountVectorizer + NGram + ChisQSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f9362-19da-4ee0-8e06-263dfbbaad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigrams(inputCol=\"Processed_Sentence\", targetCol=\"target\", n=3):\n",
    "    tokenizer = Tokenizer(inputCol=inputCol, outputCol=\"words\")\n",
    "    \n",
    "    # NGram transformations for 1, 2, ..., n grams\n",
    "    ngrams = [NGram(n=i, inputCol=\"words\", outputCol=f\"{i}_grams\") for i in range(1, n + 1)]\n",
    "    \n",
    "    # CountVectorizer and IDF for each n-gram\n",
    "    cv = [CountVectorizer(vocabSize=2**14, inputCol=f\"{i}_grams\", outputCol=f\"{i}_tf\") for i in range(1, n + 1)]\n",
    "    idf = [IDF(inputCol=f\"{i}_tf\", outputCol=f\"{i}_tfidf\", minDocFreq=5) for i in range(1, n + 1)]\n",
    "    \n",
    "    # Assembler to combine all tfidf features into a single vector\n",
    "    assembler = VectorAssembler(inputCols=[f\"{i}_tfidf\" for i in range(1, n + 1)], outputCol=\"rawFeatures\")\n",
    "    \n",
    "    # Label encoding for the target column\n",
    "    label_stringIdx = StringIndexer(inputCol=targetCol, outputCol=\"label\")\n",
    "    \n",
    "    # Chi-Square feature selection\n",
    "    selector = ChiSqSelector(numTopFeatures=2**14, featuresCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    \n",
    "    # Logistic Regression classifier\n",
    "    lr = LogisticRegression()\n",
    "    \n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer] + ngrams + cv + idf + [assembler] + [label_stringIdx] + [selector] + [lr])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53679b0-7571-4f9a-977d-0a36d92c1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suivi avec MLflow\n",
    "with mlflow.start_run():\n",
    "    # Définir le pipeline\n",
    "    pipeline = build_trigrams()\n",
    "\n",
    "    # Enregistrez les paramètres du modèle\n",
    "    mlflow.log_param(\"inputCol\", \"Processed_Sentence\")\n",
    "    mlflow.log_param(\"targetCol\", \"target\")\n",
    "    mlflow.log_param(\"nGrams\", 3)\n",
    "    mlflow.log_param(\"vocabSize\", 2**14)\n",
    "    mlflow.log_param(\"minDocFreq\", 5)\n",
    "\n",
    "    # Démarrer l'entraînement\n",
    "    start_time = datetime.utcnow()\n",
    "    pipelineFit4 = pipeline.fit(train_set)\n",
    "    training_time = datetime.utcnow() - start_time\n",
    "    print(f'Training time: {training_time}')\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit4.transform(test_set)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    # Enregistrer les métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    \n",
    "    # Enregistrer le modèle dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit4, \"model\")\n",
    "    \n",
    "    # Enregistrer le temps d'entraînement dans MLflow\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time.total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ba3ef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Metrics with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155fbe6-534f-44e0-8e19-fdb58aa7e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la fonction pour construire les trigrammes\n",
    "def build_trigrams(inputCol=[\"Processed_Sentence\",\"target\"], n=3):\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")]\n",
    "    \n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**14, inputCol=\"{0}_grams\".format(i), outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"rawFeatures\"\n",
    "    )]\n",
    "    \n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    \n",
    "    selector = [ChiSqSelector(numTopFeatures=2**14, featuresCol='rawFeatures', outputCol=\"features\")]\n",
    "    \n",
    "    lr = [LogisticRegression(regParam = 0.1, maxIter = 1000, elasticNetParam = 0.0)]\n",
    "    \n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf + assembler + label_stringIdx + selector + lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e1d4d-4bb5-4eae-b2a7-2f38209e8cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commencez une session MLflow\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Loggez les hyperparamètres utilisés\n",
    "    mlflow.log_param(\"ngrams\", \"Trigrams\")\n",
    "    mlflow.log_param(\"cv_vocab_size\", 2**14)\n",
    "    mlflow.log_param(\"idf_minDocFreq\", 5)\n",
    "    mlflow.log_param(\"regParam\", 0.1)\n",
    "    mlflow.log_param(\"maxIter\", 1000)\n",
    "    mlflow.log_param(\"elasticNetParam\", 0.0)\n",
    "\n",
    "    # Démarrer le processus de construction et d'entraînement du modèle\n",
    "    start = time.time()\n",
    "\n",
    "    pipelineFit5 = build_trigrams().fit(train_set)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit5.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "    \n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Enregistrer les métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Enregistrer le modèle entraîné dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit5, \"model\")\n",
    "\n",
    "    # Loggez le temps d'entraînement\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    print(\"Training time:\", training_time)\n",
    "\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e2956-7332-4588-a3a1-a4a95669e552",
   "metadata": {},
   "source": [
    "<h1>Naive Bayes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d40d3e-ac8d-484d-920a-0ea11a73c871",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (paramètres par défaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be45c1-bcbf-430c-8e8d-e7b101d1bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commencez une session MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Loggez les hyperparamètres utilisés\n",
    "    mlflow.log_param(\"hashingTF_inputCol\", \"words\")\n",
    "    mlflow.log_param(\"hashingTF_outputCol\", \"tf\")\n",
    "    mlflow.log_param(\"idf_inputCol\", \"tf\")\n",
    "    mlflow.log_param(\"idf_outputCol\", \"features\")\n",
    "    mlflow.log_param(\"naiveBayes_modelType\", \"multinomial\")\n",
    "\n",
    "    # Définir les étapes du pipeline\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    hashtf = HashingTF(inputCol=\"words\", outputCol='tf')\n",
    "    idf = IDF(inputCol='tf', outputCol=\"features\")\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    nb = NaiveBayes(modelType=\"multinomial\")\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, nb])\n",
    "\n",
    "    # Démarrer le processus de construction et d'entraînement du modèle\n",
    "    start = time.time()\n",
    "\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Enregistrer les métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Enregistrer le modèle entraîné dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")\n",
    "\n",
    "    # Loggez le temps d'entraînement\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    print(\"Training time:\", training_time)\n",
    "\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e0a288-548c-4a10-b4a4-9d7474e5d01a",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (paramètres customisés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a67be5-b9da-4857-a3bc-8193d111fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démarrez une session MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Loggez les hyperparamètres\n",
    "    mlflow.log_param(\"hashingTF_numFeatures\", 2**16)\n",
    "    mlflow.log_param(\"idf_minDocFreq\", 5)\n",
    "    mlflow.log_param(\"naiveBayes_modelType\", \"multinomial\")\n",
    "\n",
    "    # Définir les étapes du pipeline\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "    idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    nb = NaiveBayes(modelType=\"multinomial\")\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Construire le pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, nb])\n",
    "\n",
    "    # Démarrer le processus d'entraînement\n",
    "    start = time.time()\n",
    "\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Enregistrer les métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Enregistrer le modèle dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")\n",
    "\n",
    "    # Loggez le temps d'entraînement\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    print(\"Training time:\", training_time)\n",
    "\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6660b42-58fd-4c5b-8f07-d51c5280d08b",
   "metadata": {},
   "source": [
    "## CountVectorizer - IDF (paramètres par défaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de9358-712a-4fb0-8c34-9fc6349520aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démarrez une session MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Loggez les hyperparamètres\n",
    "    mlflow.log_param(\"cv_vocabSize\", 2**16)\n",
    "    mlflow.log_param(\"idf_minDocFreq\", 5)\n",
    "    mlflow.log_param(\"naiveBayes_modelType\", \"multinomial\")\n",
    "\n",
    "    # Définir les étapes du pipeline\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    cv = CountVectorizer(inputCol=\"words\", outputCol='cv')\n",
    "    idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    nb = NaiveBayes(modelType=\"multinomial\")\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Construire le pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, nb])\n",
    "\n",
    "    # Démarrer le processus d'entraînement\n",
    "    start = time.time()\n",
    "\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Enregistrer les métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Enregistrer le modèle dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")\n",
    "\n",
    "    # Loggez le temps d'entraînement\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    print(\"Training time:\", training_time)\n",
    "\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f109a6-fad7-4acc-bed7-5988db1d7686",
   "metadata": {},
   "source": [
    "## CountVectorizer - IDF (paramètres customisés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe65365-5d2b-4990-95e0-8ba1cfb4bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démarrez une session MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Loggez les hyperparamètres\n",
    "    mlflow.log_param(\"cv_vocabSize\", 2**16)\n",
    "    mlflow.log_param(\"idf_minDocFreq\", 5)\n",
    "    mlflow.log_param(\"naiveBayes_modelType\", \"multinomial\")\n",
    "\n",
    "    # Définir les étapes du pipeline\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "    idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    nb = NaiveBayes(modelType=\"multinomial\")\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Construire le pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, nb])\n",
    "\n",
    "    # Démarrer le processus d'entraînement\n",
    "    start = time.time()\n",
    "\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Enregistrer les métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Enregistrer le modèle dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")\n",
    "\n",
    "    # Loggez le temps d'entraînement\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    print(\"Training time:\", training_time)\n",
    "\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b6428-7729-435e-8b8d-7941dbf1a7e3",
   "metadata": {},
   "source": [
    "# CountVectorizer + NGram + ChisQSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fd419-639f-49b3-8259-9c60a1b38268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigrams(inputCol=[\"Processed_Sentence\",\"target\"], n=3):\n",
    "    \n",
    "    # Tokenizer for the sentences\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    \n",
    "    # Generating NGrams from 1 to n\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    # CountVectorizer for each nGram\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**14, inputCol=\"{0}_grams\".format(i), outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    # IDF for each count vectorizer\n",
    "    idf = [\n",
    "        IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5)\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    # VectorAssembler to combine all features\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"rawFeatures\"\n",
    "    )\n",
    "    \n",
    "    # StringIndexer for the target variable\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "    \n",
    "    # Chi-squared selector for feature selection\n",
    "    selector = ChiSqSelector(numTopFeatures=2**14, featuresCol='rawFeatures', outputCol=\"features\")\n",
    "    \n",
    "    # NaiveBayes classifier\n",
    "    nb = NaiveBayes(modelType=\"multinomial\", labelCol=\"label\", featuresCol=\"features\")\n",
    "    \n",
    "    # Combine all stages in the pipeline\n",
    "    stages = [tokenizer] + ngrams + cv + idf + [assembler] + [label_stringIdx] + [selector] + [nb]\n",
    "    \n",
    "    # Return the pipeline\n",
    "    return Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fe8e7-d6dd-475c-99a3-7ca9751557e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Integration\n",
    "with mlflow.start_run():\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"n_grams\", 3)\n",
    "    mlflow.log_param(\"cv_vocabSize\", 2**14)\n",
    "    mlflow.log_param(\"idf_minDocFreq\", 5)\n",
    "    mlflow.log_param(\"chiSqSelector_numTopFeatures\", 2**14)\n",
    "    mlflow.log_param(\"naiveBayes_modelType\", \"multinomial\")\n",
    "\n",
    "    # Build and fit the pipeline\n",
    "    pipeline = build_trigrams()\n",
    "    \n",
    "    start = time.time()\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "    end = time.time()\n",
    "\n",
    "    # Log training time\n",
    "    training_time = end - start\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "    print(\"Training time:\", training_time)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Evaluate metrics\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Save the model to MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ecf1b8-0f92-452a-9272-b940e8315b2f",
   "metadata": {},
   "source": [
    "## Cross-Validation 10-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c99450-f283-4a0e-b6a4-8b89a82fdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Integration\n",
    "with mlflow.start_run():\n",
    "    # Tokenizer\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    \n",
    "    # CountVectorizer\n",
    "    cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "    \n",
    "    # IDF\n",
    "    idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "    \n",
    "    # StringIndexer\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "    \n",
    "    # Naive Bayes\n",
    "    nb = NaiveBayes(modelType=\"multinomial\")\n",
    "    \n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, nb])\n",
    "    \n",
    "    # Param Grid for Cross-Validation\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "        .build()\n",
    "    \n",
    "    # CrossValidator\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "    cv_model = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=10\n",
    "    )\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"cv_vocabSize\", 2**16)\n",
    "    mlflow.log_param(\"idf_minDocFreq\", 5)\n",
    "    mlflow.log_param(\"naiveBayes_modelType\", \"multinomial\")\n",
    "    mlflow.log_param(\"cv_numFolds\", 10)\n",
    "    mlflow.log_param(\"nb_smoothing_values\", [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    \n",
    "    # Fit the model\n",
    "    start = time.time()\n",
    "    pipelineFit = cv_model.fit(train_set)\n",
    "    end = time.time()\n",
    "    \n",
    "    # Log training time\n",
    "    training_time = end - start\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "    print(\"Training time:\", training_time)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    \n",
    "    # Best model\n",
    "    best_model = pipelineFit.bestModel\n",
    "    best_params = pipelineFit.getEstimatorParamMaps()[np.argmax(pipelineFit.avgMetrics)]\n",
    "    \n",
    "    # Log best parameters and model\n",
    "    mlflow.log_param(\"best_params\", best_params)\n",
    "    mlflow.spark.log_model(best_model, \"best_model\")\n",
    "    \n",
    "    # Print metrics and best parameters\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07667519-535f-4b47-b75a-ea01cb455052",
   "metadata": {},
   "source": [
    "<h1>SVM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d15e7-6722-426d-b70c-785b02e1beda",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (paramètres par défaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e6d81-981b-42cc-82c0-a52e9f1debaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démarrer une session MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Log des hyperparamètres\n",
    "    mlflow.log_param(\"hashingTF_numFeatures\", \"default\")  # Par défaut, HashingTF utilise 2^20 features\n",
    "    mlflow.log_param(\"idf_minDocFreq\", \"default\")  # Si non défini, la valeur par défaut est 0\n",
    "    mlflow.log_param(\"svm_maxIter\", 100)\n",
    "    mlflow.log_param(\"svm_regParam\", 0.01)\n",
    "\n",
    "    # Définir les étapes du pipeline\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    hashtf = HashingTF(inputCol=\"words\", outputCol='tf')\n",
    "    idf = IDF(inputCol='tf', outputCol=\"features\")\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    # Définir le classifieur SVM et OneVsRest\n",
    "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100, regParam=0.01)\n",
    "    ovr = OneVsRest(classifier=svm)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Construire le pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, ovr])\n",
    "\n",
    "    # Démarrer le processus d'entraînement\n",
    "    start = time.time()\n",
    "\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "    # Faire des prédictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Calculer les métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Log des métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Enregistrer le modèle dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")\n",
    "\n",
    "    # Log du temps d'entraînement\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    print(\"Training time:\", training_time)\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5bfddc-f6b2-4843-a36a-fc907b26bd23",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (paramètres customisés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1268300-293a-4c76-8df6-399dd383c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 2**16  # Nombre de caractéristiques pour HashingTF\n",
    "min_doc_freq = 5      # Fréquence minimale des documents pour IDF\n",
    "\n",
    "# Démarrer une session MLflow\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Log des hyperparamètres\n",
    "    mlflow.log_param(\"hashingTF_numFeatures\", num_features)\n",
    "    mlflow.log_param(\"idf_minDocFreq\", min_doc_freq)\n",
    "\n",
    "    # Construction du pipeline\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    hashtf = HashingTF(numFeatures=num_features, inputCol=\"words\", outputCol='tf')\n",
    "    idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=min_doc_freq)\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    # Définir le classifieur\n",
    "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100, regParam=0.01)\n",
    "    ovr = OneVsRest(classifier=svm)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Construire le pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, ovr])\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    start = time.time()\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "\n",
    "    # Prédictions sur le jeu de test\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Log des métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Imprimer les résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Sauvegarder le modèle dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c28884-c885-49ae-b2e1-344be9d66dcc",
   "metadata": {},
   "source": [
    "## CountVectorizer - IDF (paramètres par défaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae20b6-adef-4df1-b35e-0afe0599b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démarrer une session MLflow\n",
    "with mlflow.start_run():\n",
    "    # Log des hyperparamètres (par défaut pour CountVectorizer et IDF)\n",
    "    mlflow.log_param(\"CountVectorizer_vocabularySize\", \"default\")\n",
    "    mlflow.log_param(\"IDF_minDocFreq\", \"default\")\n",
    "\n",
    "    # Construction du pipeline\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    cv = CountVectorizer(inputCol=\"words\", outputCol='cv')  # Paramètres par défaut\n",
    "    idf = IDF(inputCol='cv', outputCol=\"features\")          # Paramètres par défaut\n",
    "\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    # Définir le classifieur\n",
    "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100, regParam=0.01)\n",
    "    ovr = OneVsRest(classifier=svm)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Construire le pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, ovr])\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    start = time.time()\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "\n",
    "    # Prédictions sur le jeu de test\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Log des métriques dans MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Imprimer les résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Sauvegarder le modèle dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc611d-ec16-416b-ae4c-80a77a13c292",
   "metadata": {},
   "source": [
    "## CountVectorizer - IDF (paramètres customisés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9335596-a833-4348-8dae-42d1477728cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démarrer une session MLflow\n",
    "with mlflow.start_run():\n",
    "    # Log des hyperparamètres personnalisés\n",
    "    mlflow.log_param(\"CountVectorizer_vocabSize\", 2**16)\n",
    "    mlflow.log_param(\"IDF_minDocFreq\", 5)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(inputCol=\"Processed_Sentence\", outputCol=\"words\")\n",
    "    \n",
    "    # CountVectorizer avec des paramètres personnalisés\n",
    "    cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "    \n",
    "    # IDF avec suppression des termes rares\n",
    "    idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "    # Conversion de la cible en index numérique\n",
    "    label_stringIdx = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "\n",
    "    # Classifieur SVM pour One-vs-Rest\n",
    "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100, regParam=0.01)\n",
    "    ovr = OneVsRest(classifier=svm)\n",
    "\n",
    "    # Évaluateur pour les métriques\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, ovr])\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    start = time.time()\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "\n",
    "    # Prédictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "    # Évaluation des performances\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Log des métriques\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Sauvegarder le modèle dans MLflow\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5dfaea-10db-46cd-a81d-6478ae7209db",
   "metadata": {},
   "source": [
    "# CountVectorizer + NGram + ChisQSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c95adc-2972-499e-a494-e429f64178ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigrams(inputCol=(\"Processed_Sentence\", \"target\"), n=3):\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(inputCol=inputCol[0], outputCol=\"words\")\n",
    "    \n",
    "    # Generate n-grams, CountVectorizer, and IDF for each n\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=f\"{i}_grams\")\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**14, inputCol=f\"{i}_grams\", outputCol=f\"{i}_tf\")\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    idf = [\n",
    "        IDF(inputCol=f\"{i}_tf\", outputCol=f\"{i}_tfidf\", minDocFreq=5)\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    # Assemble all TF-IDF features into a single feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[f\"{i}_tfidf\" for i in range(1, n + 1)],\n",
    "        outputCol=\"rawFeatures\"\n",
    "    )\n",
    "    \n",
    "    # Index the label column\n",
    "    label_stringIdx = StringIndexer(inputCol=inputCol[1], outputCol=\"label\")\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = ChiSqSelector(\n",
    "        numTopFeatures=2**14, featuresCol=\"rawFeatures\", outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    # SVM with One-vs-Rest for multiclass classification\n",
    "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\")\n",
    "    ovr = OneVsRest(classifier=svm)\n",
    "    \n",
    "    # Return the constructed pipeline\n",
    "    return Pipeline(stages=[tokenizer] + ngrams + cv + idf + [assembler, label_stringIdx, selector, ovr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e87f1-1716-4ef2-bce5-ea2d0f733c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suivi avec MLflow\n",
    "with mlflow.start_run():\n",
    "    # Log des hyperparamètres\n",
    "    mlflow.log_param(\"ngram_range\", 3)\n",
    "    mlflow.log_param(\"CountVectorizer_vocabSize\", 2**14)\n",
    "    mlflow.log_param(\"IDF_minDocFreq\", 5)\n",
    "    mlflow.log_param(\"ChiSqSelector_numTopFeatures\", 2**14)\n",
    "\n",
    "    # Construction et entraînement du pipeline\n",
    "    start_time = time.time()\n",
    "    pipeline = build_trigrams()\n",
    "    pipelineFit = pipeline.fit(train_set)\n",
    "    training_time = time.time() - start_time\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "    \n",
    "    # Prédictions\n",
    "    predictions = pipelineFit.transform(test_set)\n",
    "    \n",
    "    # Évaluation des performances\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    \n",
    "    # Log des métriques\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    mlflow.spark.log_model(pipelineFit, \"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
